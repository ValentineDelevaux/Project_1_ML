{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5029a08d-b918-496e-95f7-3dd7d78ede71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16566bcc-2237-47a3-8a87-d87da84777b1",
   "metadata": {},
   "source": [
    "## Mean squared error gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c914a656-b864-4703-8323-520e42c48aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # TODO: compute loss by MSE\n",
    "\n",
    "    loss = (1/ 2) * np.mean((y-tx.dot(w))**2)\n",
    "   \n",
    "    return loss\n",
    "\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # TODO: compute gradient vector\n",
    "    \n",
    "    error = y - tx.dot(w)\n",
    "    gradient = (-1/y.shape[0]) * tx.T.dot(error)\n",
    "    \n",
    "    return gradient\n",
    "    # ***************************************************\n",
    "    \n",
    "\n",
    "\n",
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        initial_w: shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # TODO: compute gradient and loss\n",
    "        \n",
    "        loss =  compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # TODO: update w by gradient\n",
    "        \n",
    "        w = w - np.dot(gamma, gradient)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # store w and loss\n",
    "        \n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5806ff-08bd-4d73-9f2c-d4640149d875",
   "metadata": {},
   "source": [
    "## Mean squared error stochastic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e1a3689-5243-48d8-81c9-9edd6871e4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35bd4e09-b835-4936-8be2-dbe4a199329d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    \n",
    "    error = y - tx.dot(w)\n",
    "    stoch_gradient = (-1 / y.shape[0]) * tx.T.dot(error)\n",
    "    \n",
    "    return stoch_gradient\n",
    "\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def mean_squared_error_sgd(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        initial_w: shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    \n",
    "    # ***************************************************\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "        \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    " \n",
    "            w = w - np.multiply(gamma, compute_stoch_gradient(minibatch_y, minibatch_tx, w))\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        # ***************************************************\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee7399-e7b9-4527-b5a3-30f3bc6b572b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a833859-a8f2-4c9c-8cce-44cc1c08ff78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "\n",
    "    >>> least_squares(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]))\n",
    "    (array([ 0.21212121, -0.12121212]), 8.666684749742561e-33)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # least squares: TODO\n",
    "    # returns mse, and optimal weights\n",
    "    \n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a,b)\n",
    "    \n",
    "    loss = compute_loss(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023df7a-311a-4b7d-b448-b5fe2db9115e",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f15dac07-2ad5-4b29-933a-b5bac135a75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 0)\n",
    "    array([ 0.21212121, -0.12121212])\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 1)\n",
    "    array([0.03947092, 0.00319628])\n",
    "    \"\"\"\n",
    "    lambda_prime = 2 * tx.shape[0] * lambda_\n",
    "    \n",
    "    w = np.linalg.solve(tx.T.dot(tx) + lambda_prime * np.identity(tx.shape[1]), tx.T.dot(y)  )\n",
    "    loss = compute_loss(y, tx, w)\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a72752-e674-4dac-85da-6a18139c56ea",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dd9a5f8-bfb5-4bb7-b8db-e944e7c7f96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "\n",
    "    >>> sigmoid(np.array([0.1]))\n",
    "    array([0.52497919])\n",
    "    >>> sigmoid(np.array([0.1, 0.1]))\n",
    "    array([0.52497919, 0.52497919])\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/ (1 + np.exp(-t))\n",
    "\n",
    "def calculate_logistic_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a non-negative loss\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(4).reshape(2, 2)\n",
    "    >>> w = np.c_[[2., 3.]]\n",
    "    >>> round(calculate_loss(y, tx, w), 8)\n",
    "    1.52429481\n",
    "    \"\"\"\n",
    "    assert y.shape[0] == tx.shape[0]\n",
    "    assert tx.shape[1] == w.shape[0]\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "   \n",
    "    y_predicted = sigmoid(tx.dot(w))\n",
    "    \n",
    "    #loss = (1 / (tx.shape[0])) * np.sum(-y.T.dot(tx.T).dot(w) + np.log(1+ np.exp(x.dot(w))))\n",
    "    \n",
    "    loss = np.linalg.norm ( (-1 / (tx.shape[0])) * ( y.T.dot(np.log(y_predicted)) + (1 - y).T.dot(np.log(1 - y_predicted))) )\n",
    "\n",
    "    return loss\n",
    "    # ***************************************************\n",
    "\n",
    "def calculate_logistic_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> calculate_gradient(y, tx, w)\n",
    "    array([[-0.10370763],\n",
    "           [ 0.2067104 ],\n",
    "           [ 0.51712843]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    y_predicted = sigmoid(tx.dot(w))\n",
    "    grad = (1 / (tx.shape[0])) * tx.T.dot(y_predicted - y)\n",
    "    return grad\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3752e88-634d-4452-b79a-4782525c0607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression. Return the loss and the updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        gamma: float\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> w\n",
    "    array([[0.11037076],\n",
    "           [0.17932896],\n",
    "           [0.24828716]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    loss = calculate_logistic_loss(y, tx, w)\n",
    "    gradient = calculate_logistic_gradient(y, tx, w)\n",
    "    w = w - gamma * gradient\n",
    "    \n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5a570-6126-4ba0-b995-cebac42cc399",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b30bd5a-8ce8-410a-ad09-94f2eb5671ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reg_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.63537268\n",
    "    >>> gradient\n",
    "    array([[-0.08370763],\n",
    "           [ 0.2467104 ],\n",
    "           [ 0.57712843]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient: TODO\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w) )\n",
    "    gradient = calculate_gradient(y, tx, w) +  2 * lambda_ * w \n",
    "    return loss, gradient\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd7636b2-fc2f-4b4d-8c22-7aabff29419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        gamma: scalar\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.63537268\n",
    "    >>> w\n",
    "    array([[0.10837076],\n",
    "           [0.17532896],\n",
    "           [0.24228716]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # return loss, gradient: \n",
    "    loss, gradient = compute_reg_logistic_regression(y, tx, w, lambda_)\n",
    "    # ***************************************************\n",
    "    # update w:\n",
    "    w = w - gamma * gradient\n",
    "    # ***************************************************\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0722b3-317b-4b4f-bce0-3826ca681979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
