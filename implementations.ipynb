{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5029a08d-b918-496e-95f7-3dd7d78ede71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    \"../../Machine_Learning/projet/grading_tests\")\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "\n",
    "from test_project1_public import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16566bcc-2237-47a3-8a87-d87da84777b1",
   "metadata": {},
   "source": [
    "## Mean squared error gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c914a656-b864-4703-8323-520e42c48aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # TODO: compute loss by MSE\n",
    "\n",
    "    loss = (1/ 2) * np.mean((y-tx.dot(w))**2)\n",
    "   \n",
    "    return loss\n",
    "\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # TODO: compute gradient vector\n",
    "    \n",
    "    error = y - tx.dot(w)\n",
    "    gradient = (-1/y.shape[0]) * tx.T.dot(error)\n",
    "    \n",
    "    return gradient\n",
    "    # ***************************************************\n",
    "    \n",
    "\n",
    "\n",
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        initial_w: shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # TODO: compute gradient and loss\n",
    "        \n",
    "        loss =  compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # TODO: update w by gradient\n",
    "        \n",
    "        w = w - np.dot(gamma, gradient)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # store w and loss\n",
    "        \n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5806ff-08bd-4d73-9f2c-d4640149d875",
   "metadata": {},
   "source": [
    "## Mean squared error stochastic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e1a3689-5243-48d8-81c9-9edd6871e4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35bd4e09-b835-4936-8be2-dbe4a199329d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    \n",
    "    error = y - tx.dot(w)\n",
    "    stoch_gradient = (-1 / y.shape[0]) * tx.T.dot(error)\n",
    "    \n",
    "    return stoch_gradient\n",
    "\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def mean_squared_error_sgd(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        initial_w: shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    \n",
    "    # ***************************************************\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "        \n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    " \n",
    "            w = w - np.multiply(gamma, compute_stoch_gradient(minibatch_y, minibatch_tx, w))\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "        \n",
    "        # ***************************************************\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee7399-e7b9-4527-b5a3-30f3bc6b572b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a833859-a8f2-4c9c-8cce-44cc1c08ff78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "\n",
    "    >>> least_squares(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]))\n",
    "    (array([ 0.21212121, -0.12121212]), 8.666684749742561e-33)\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # least squares: TODO\n",
    "    # returns mse, and optimal weights\n",
    "    \n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    w = np.linalg.solve(a,b)\n",
    "    \n",
    "    loss = compute_loss(y, tx, w)\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023df7a-311a-4b7d-b448-b5fe2db9115e",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f15dac07-2ad5-4b29-933a-b5bac135a75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "\n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 0)\n",
    "    array([ 0.21212121, -0.12121212])\n",
    "    >>> ridge_regression(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), 1)\n",
    "    array([0.03947092, 0.00319628])\n",
    "    \"\"\"\n",
    "    lambda_prime = 2 * tx.shape[0] * lambda_\n",
    "    \n",
    "    w = np.linalg.solve(tx.T.dot(tx) + lambda_prime * np.identity(tx.shape[1]), tx.T.dot(y)  )\n",
    "    loss = compute_loss(y, tx, w)\n",
    "\n",
    "    return w, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
